{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b05ef39-2180-494f-a5af-9dd2abb76452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import gzip\n",
    "import html\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from utils import set_device, load_json, load_plm, clean_text\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig, AutoTokenizer, AutoModel,AutoModelForCausalLM\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def load_data(args):\n",
    "\n",
    "    item2feature_path = os.path.join(args.root, f'{args.dataset}.item.json')\n",
    "    item2feature = load_json(item2feature_path)\n",
    "\n",
    "    return item2feature\n",
    "\n",
    "def generate_text(item2feature, features):\n",
    "    item_text_list = []\n",
    "\n",
    "    for item in item2feature:\n",
    "        data = item2feature[item]\n",
    "        text = []\n",
    "        for meta_key in features:\n",
    "            if meta_key in data:\n",
    "                meta_value = clean_text(data[meta_key])\n",
    "                text.append(meta_value.strip())\n",
    "\n",
    "        item_text_list.append([int(item), text])\n",
    "\n",
    "    return item_text_list\n",
    "\n",
    "def preprocess_text(args):\n",
    "    print('Process text data: ')\n",
    "    print(' Dataset: ', args.dataset)\n",
    "\n",
    "    item2feature = load_data(args)\n",
    "    # load item text and clean\n",
    "    item_text_list = generate_text(item2feature, ['title', 'description'])\n",
    "    # item_text_list = generate_text(item2feature, ['title'])\n",
    "    # return: list of (item_ID, cleaned_item_text)\n",
    "    return item_text_list\n",
    "\n",
    "def generate_item_embedding(args, item_text_list, tokenizer, model, word_drop_ratio=-1):\n",
    "    print(f'Generate Text Embedding: ')\n",
    "    print(' Dataset: ', args.dataset)\n",
    "\n",
    "    items, texts = zip(*item_text_list)\n",
    "    order_texts = [[0]] * len(items)\n",
    "    for item, text in zip(items, texts):\n",
    "        order_texts[item] = text\n",
    "    for text in order_texts:\n",
    "        assert text != [0]\n",
    "\n",
    "    embeddings = []\n",
    "    start, batch_size = 0, 1\n",
    "    while start < len(order_texts):\n",
    "        if (start+1)%100==0:\n",
    "            print(\"==>\",start+1)\n",
    "        field_texts = order_texts[start: start + batch_size]\n",
    "        # print(field_texts)\n",
    "        field_texts = zip(*field_texts)\n",
    "\n",
    "        field_embeddings = []\n",
    "        for sentences in field_texts:\n",
    "            sentences = list(sentences)\n",
    "            # print(sentences)\n",
    "            if word_drop_ratio > 0:\n",
    "                print(f'Word drop with p={word_drop_ratio}')\n",
    "                new_sentences = []\n",
    "                for sent in sentences:\n",
    "                    new_sent = []\n",
    "                    sent = sent.split(' ')\n",
    "                    for wd in sent:\n",
    "                        rd = random.random()\n",
    "                        if rd > word_drop_ratio:\n",
    "                            new_sent.append(wd)\n",
    "                    new_sent = ' '.join(new_sent)\n",
    "                    new_sentences.append(new_sent)\n",
    "                sentences = new_sentences\n",
    "            encoded_sentences = tokenizer(sentences, max_length=args.max_sent_len,\n",
    "                                          truncation=True, return_tensors='pt',padding=\"longest\").to(args.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=encoded_sentences.input_ids,\n",
    "                                attention_mask=encoded_sentences.attention_mask)\n",
    "                # print(\"Start:\", str(start))\n",
    "            masked_output = outputs.last_hidden_state * encoded_sentences['attention_mask'].unsqueeze(-1)\n",
    "            mean_output = masked_output.sum(dim=1) / encoded_sentences['attention_mask'].sum(dim=-1, keepdim=True)\n",
    "            mean_output = mean_output.detach().cpu()\n",
    "            field_embeddings.append(mean_output)\n",
    "            \n",
    "        field_mean_embedding = torch.stack(field_embeddings, dim=0).mean(dim=0)\n",
    "        embeddings.append(field_mean_embedding)\n",
    "        start += batch_size\n",
    "\n",
    "    embeddings = torch.cat(embeddings, dim=0).numpy()\n",
    "    print('Embeddings shape: ', embeddings.shape)\n",
    "\n",
    "    file = os.path.join(args.root, args.dataset + '.emb-' + args.plm_name + \"-td\" + \".npy\")\n",
    "    np.save(file, embeddings)\n",
    "\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--dataset', type=str, default='Instruments', help='Instruments / Arts / Games')\n",
    "#     parser.add_argument('--root', type=str, default=\"\")\n",
    "#     parser.add_argument('--gpu_id', type=int, default=2, help='ID of running GPU')\n",
    "#     parser.add_argument('--plm_name', type=str, default='llama')\n",
    "#     parser.add_argument('--plm_checkpoint', type=str,\n",
    "#                         default='')\n",
    "#     parser.add_argument('--max_sent_len', type=int, default=2048)\n",
    "#     parser.add_argument('--word_drop_ratio', type=float, default=-1, help='word drop ratio, do not drop by default')\n",
    "#     return parser.parse_args()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     args = parse_args()\n",
    "#     args.root = os.path.join(args.root, args.dataset)\n",
    "\n",
    "#     device = set_device(args.gpu_id)\n",
    "#     args.device = device\n",
    "\n",
    "#     item_text_list = preprocess_text(args)\n",
    "\n",
    "#     plm_tokenizer, plm_model = load_plm(args.plm_checkpoint)\n",
    "#     if plm_tokenizer.pad_token_id is None:\n",
    "#         plm_tokenizer.pad_token_id = 0\n",
    "#     plm_model = plm_model.to(device)\n",
    "\n",
    "#     generate_item_embedding(args, item_text_list,plm_tokenizer,\n",
    "#                             plm_model, word_drop_ratio=args.word_drop_ratio)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99821bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.48it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = '/root/autodl-tmp/qwen7b/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,low_cpu_mem_usage=True,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9035460c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'who', b' am', b' i']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.tokenize('who am i', max_length=100,\n",
    "                                          truncation=True, return_tensors='pt',padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "113bf942",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc23b2ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerBase.pad() missing 1 required positional argument: 'encoded_inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: PreTrainedTokenizerBase.pad() missing 1 required positional argument: 'encoded_inputs'"
     ]
    }
   ],
   "source": [
    "tokenizer.pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57f9106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c12b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings = np.load('/root/autodl-tmp/data/fashion/fashion.emb-llama-td.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8e35252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "lle = LocallyLinearEmbedding(n_neighbors=30,n_components=768)\n",
    "lle_item_emb = lle.fit_transform(embeddings.reshape(-1,4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ff4f61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00302897, 0.01810408, 0.01983337, ..., 0.90955505, 0.94262341,\n",
       "       0.94928422], shape=(22660,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lle_item_emb*lle_item_emb).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b841339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00043163, 0.00042918, 0.00042901, ..., 0.00042995, 0.00043029,\n",
       "       0.00043029], shape=(22660,), dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pca_item_emb*pca_item_emb).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35192220",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=768)\n",
    "pca_item_emb = pca.fit_transform(embeddings.reshape(-1,4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1a7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/LETTER/data/Instruments/Instruments.emb-llama-td-pca.npy',pca_item_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0f97e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06546738,  0.07035364,  0.12352964, ..., -0.05386544,\n",
       "        -0.01489884, -0.05661311],\n",
       "       [-0.00936208,  0.0402041 , -0.2232153 , ..., -0.02281781,\n",
       "        -0.0177921 ,  0.060487  ],\n",
       "       [-0.10918454, -0.11901627, -0.17798027, ..., -0.07504256,\n",
       "        -0.08569156,  0.06216739],\n",
       "       ...,\n",
       "       [ 0.2804811 ,  0.3421658 , -0.07818857, ..., -0.14269552,\n",
       "         0.03339064,  0.03802712],\n",
       "       [ 0.18306674,  0.27125368, -0.01843184, ..., -0.02817823,\n",
       "         0.05040237, -0.00770262],\n",
       "       [-0.0802355 ,  0.22407892,  0.2701178 , ...,  0.06522095,\n",
       "         0.33446762, -0.08688183]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('/root/LETTER/RQ-VAE/ckpt/Instruments-32d-sasrec.pt').squeeze().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708d524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f753b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.root = os.path.join(args.root, args.dataset)\n",
    "\n",
    "device = set_device(0)\n",
    "\n",
    "item_text_list = preprocess_text(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbeef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = '1'\n",
    "encoded_sentences = tokenizer(sentences, max_length=2048,\n",
    "                                          truncation=True, return_tensors='pt',padding=\"longest\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=encoded_sentences.input_ids,\n",
    "                    attention_mask=encoded_sentences.attention_mask,output_last_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49ed4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.pad_token = '<|endoftext|>'\n",
    "tokenizer.pad_token_id = 151643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "122d2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=encoded_sentences.input_ids,\n",
    "                    attention_mask=encoded_sentences.attention_mask,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f26cba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2188,  3.4844,  3.7500,  ..., -3.7656,  0.6797, -2.2969],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[-1][0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f6c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
