{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py310/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add 2064 new token.\n",
      "data num: 12547\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "from typing import List\n",
    "from modeling_letter import LETTER\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn \n",
    "# from peft import PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig, T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "import types\n",
    "from utils import *\n",
    "from collator import TestCollator, Collator\n",
    "from evaluate import get_topk_results, get_metrics_results\n",
    "from generation_trie import Trie\n",
    "\n",
    "class SimpleArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "args = SimpleArgs(\n",
    "    seed=42,\n",
    "    gpu_id=0,\n",
    "    tasks=\"seqrec\",\n",
    "    dataset=\"fashion\",\n",
    "    index_file=\".index.epoch10000.alpha0-beta1e-4.json\",\n",
    "    data_path=\"/root/autodl-tmp/data\",\n",
    "    his_sep=\", \",\n",
    "    max_his_len=20,\n",
    "    only_train_response=False,\n",
    "    add_prefix=False,\n",
    "    train_prompt_sample_num=\"1\",\n",
    "    train_data_sample_num=\"-1\",\n",
    "    valid_prompt_id=0,\n",
    "    sample_valid=True,\n",
    "    valid_prompt_sample_num=2,\n",
    "    ckpt_path=\"./ckpt/Instruments/checkpoint-3465\",\n",
    "    results_file=\"./results/$DATASET/res.json\",\n",
    "    test_task=\"SeqRec\",\n",
    "    test_prompt_ids=\"0\",\n",
    "    sample_num=-1,\n",
    "    num_beams=20,\n",
    "    test_batch_size=20,\n",
    ")\n",
    "set_seed(args.seed)\n",
    "\n",
    "device_map = {\"\": args.gpu_id}\n",
    "device = torch.device(\"cuda\",args.gpu_id)\n",
    "\n",
    "config = T5Config.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    \"t5-small\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "train_data, valid_data = load_datasets(args)\n",
    "add_num = tokenizer.add_tokens(train_data.datasets[0].get_new_tokens())\n",
    "config.vocab_size = len(tokenizer)\n",
    "\n",
    "print(\"add {} new token.\".format(add_num))\n",
    "print(\"data num:\", len(train_data))\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(args.ckpt_path)\n",
    "model = LETTER(config)\n",
    "#model.set_hyper(args.temperature)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\n",
    "#     args.ckpt_path,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     device_map=device_map,\n",
    "# )\n",
    "prompt_ids = [0]\n",
    "\n",
    "test_data = load_test_dataset(args)\n",
    "\n",
    "\n",
    "# collator = TestCollator(args, tokenizer)\n",
    "collator = Collator(args, tokenizer)\n",
    "all_items = test_data.get_all_items()\n",
    "\n",
    "\n",
    "\n",
    "candidate_trie = Trie(\n",
    "        [\n",
    "            [0] + tokenizer.encode(candidate)\n",
    "            for candidate in all_items\n",
    "        ]\n",
    "    )\n",
    "prefix_allowed_tokens = prefix_allowed_tokens_fn(candidate_trie)\n",
    "train_loader = DataLoader(train_data,collate_fn=collator,batch_size=20)\n",
    "# test_loader = DataLoader(test_data, batch_size=args.test_batch_size, collate_fn=collator,\n",
    "                            # shuffle=True, num_workers=4, pin_memory=True)\n",
    "for i in train_loader:\n",
    "    break\n",
    "input_ids = i['input_ids']\n",
    "attention_mask = i['attention_mask']\n",
    "actions = i['labels'].unsqueeze(1).repeat(1,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(input_ids=i['input_ids'].to(model.device),attention_mask=i['attention_mask'].to(model.device),labels=i['labels'].to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 6, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.decoder_hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Seq2SeqLMOutput' object has no attribute 'decoder_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder_hidden_state\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'Seq2SeqLMOutput' object has no attribute 'decoder_hidden_state'"
     ]
    }
   ],
   "source": [
    "res.decoder_hidden_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
